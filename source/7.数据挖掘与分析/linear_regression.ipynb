{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "df_boston = pd.DataFrame(X)\n",
    "df_boston.columns = boston.feature_names\n",
    "df_boston[\"MEDV\"] = y\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR) #查看数据集的描述信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大最小标准化\n",
    "def linear_scale(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    scale = max_val - min_val\n",
    "    return series.apply(lambda x: ((x - min_val) / scale))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对每个特征进行标准化处理\n",
    "def normalize_linear_scale(examples_dataframe):\n",
    "    processed_features = pd.DataFrame()\n",
    "    for feature in boston.feature_names:\n",
    "        processed_features[feature] = linear_scale(examples_dataframe[feature])\n",
    "    processed_features[\"MEDV\"] = linear_scale(examples_dataframe[\"MEDV\"])\n",
    "    return processed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.641607</td>\n",
       "      <td>0.269203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089680</td>\n",
       "      <td>0.422222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.368889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>0.063466</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.994276</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.631111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "0  0.000000  0.18  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
       "1  0.000236  0.00  0.242302   0.0  0.172840  0.547998  0.782698  0.348962   \n",
       "2  0.000236  0.00  0.242302   0.0  0.172840  0.694386  0.599382  0.348962   \n",
       "3  0.000293  0.00  0.063050   0.0  0.150206  0.658555  0.441813  0.448545   \n",
       "4  0.000705  0.00  0.063050   0.0  0.150206  0.687105  0.528321  0.448545   \n",
       "\n",
       "        RAD       TAX   PTRATIO         B     LSTAT      MEDV  \n",
       "0  0.000000  0.208015  0.287234  1.000000  0.089680  0.422222  \n",
       "1  0.043478  0.104962  0.553191  1.000000  0.204470  0.368889  \n",
       "2  0.043478  0.104962  0.553191  0.989737  0.063466  0.660000  \n",
       "3  0.086957  0.066794  0.648936  0.994276  0.033389  0.631111  \n",
       "4  0.086957  0.066794  0.648936  1.000000  0.099338  0.693333  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df_boston = normalize_linear_scale((df_boston))\n",
    "normalized_df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分离数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "features=normalized_df_boston[normalized_df_boston.loc[:,normalized_df_boston.columns!='MEDV'].columns]\n",
    "target = normalized_df_boston['MEDV']\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "y_pred = lr_model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1060176631836513"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE是预测值与真实值的误差平方根的均值\n",
    "import math\n",
    "from sklearn import metrics\n",
    "math.sqrt(metrics.mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076324635523346548"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    classification\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    pairwise_fast\n",
      "    ranking\n",
      "    regression\n",
      "    scorer\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the correctly classified samples\n",
      "            (float), else it returns the number of correctly classified samples\n",
      "            (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jaccard_similarity_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_similarity_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred)\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float(upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        mutual_information_score: Mutual Information (not adjusted for chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ari : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not always pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [Hubert1985] `L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985`\n",
      "          http://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "    \n",
      "    auc(x, y, reorder=False)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array, shape = [n]\n",
      "            x coordinates.\n",
      "        y : array, shape = [n]\n",
      "            y coordinates.\n",
      "        reorder : boolean, optional (default=False)\n",
      "            If True, assume that the curve is ascending in the case of ties, as for\n",
      "            an ROC curve. If the curve is non-ascending, the result will be wrong.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Computes the area under the ROC curve\n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "    \n",
      "    average_precision_score(y_true, y_score, average='macro', sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels in binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <http://en.wikipedia.org/wiki/Average_precision>`_\n",
      "        .. [2] `Stanford Information Retrieval book\n",
      "                <http://nlp.stanford.edu/IR-book/html/htmledition/\n",
      "                evaluation-of-ranked-retrieval-results-1.html>`_\n",
      "        .. [3] `The PASCAL Visual Object Classes (VOC) Challenge\n",
      "                <http://citeseerx.ist.psu.edu/viewdoc/\n",
      "                download?doi=10.1.1.157.5766&rep=rep1&type=pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Area under the ROC curve\n",
      "        \n",
      "        precision_recall_curve :\n",
      "            Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)  # doctest: +ELLIPSIS\n",
      "        0.83...\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score.\n",
      "        \n",
      "        The smaller the Brier score, the better, hence the naming with \"loss\".\n",
      "        \n",
      "        Across all items in a set N predictions, the Brier score measures the\n",
      "        mean squared difference between (1) the predicted probability assigned\n",
      "        to the possible outcomes for item i, and (2) the actual outcome.\n",
      "        Therefore, the lower the Brier score is for a set of predictions, the\n",
      "        better the predictions are calibrated. Note that the Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1).\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter pos_label, which defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calibration>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array, shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class. If None, the maximum label is used as\n",
      "            positive class\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob,                          pos_label=\"ham\")  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score.\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_\n",
      "    \n",
      "    calinski_harabaz_score(X, labels)\n",
      "        Compute the Calinski and Harabaz score.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabaz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape (``n_samples``, ``n_features``)\n",
      "            List of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like, shape (``n_samples``,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabaz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <http://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2)\n",
      "        Build a text report showing the main classification metrics\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels]\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of strings\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int\n",
      "            Number of digits for formatting output floating point values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "        \n",
      "            The reported averages are a prevalence-weighted macro-average across\n",
      "            classes (equivalent to :func:`precision_recall_fscore_support` with\n",
      "            ``average='weighted'``).\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                     precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "            class 0       0.50      1.00      0.67         1\n",
      "            class 1       0.00      0.00      0.00         1\n",
      "            class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "        avg / total       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array, shape = [n_samples]\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array, shape = [n_samples]\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : str, optional\n",
      "            List of weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596.\n",
      "               <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa.\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          1.0\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` but\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If none is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : array, shape = [n_classes, n_classes]\n",
      "            Confusion matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : string or function, optional, default: \"jaccard\"\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, sample_weight=None)\n",
      "        Coverage error measure\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    dcg_score(y_true, y_score, k=5)\n",
      "        Discounted cumulative gain (DCG) at rank K.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (true relevance labels).\n",
      "        y_score : array, shape = [n_samples]\n",
      "            Predicted scores.\n",
      "        k : int\n",
      "            Rank.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Discounted Cumulative Gain\n",
      "               <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "    \n",
      "    euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation, and\n",
      "        the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples_1, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)\n",
      "        \n",
      "        Y_norm_squared : array-like, shape (n_samples_2, ), optional\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "        \n",
      "        squared : boolean, optional\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like, shape = [n_samples_1], optional\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : {array, sparse matrix}, shape (n_samples_1, n_samples_2)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[ 0.,  1.],\n",
      "               [ 1.,  0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[ 1.        ],\n",
      "               [ 1.41421356]])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        paired_distances : distances betweens pairs of elements of X and Y.\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the weighted average of\n",
      "        the F1 score of each class.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([ 0.8,  0. ,  0. ])\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F-beta score\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of precision in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Weight of precision in harmonic mean.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([ 0.71...,  0.        ,  0.        ])\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str | callable\n",
      "            scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, labels=None, sample_weight=None, classes=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels], optional (default=None)\n",
      "            Integer array of labels. If not provided, labels will be inferred\n",
      "            from y_true and y_pred.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        classes : array, shape = [n_labels], optional\n",
      "            Integer array of labels.\n",
      "        \n",
      "            .. deprecated:: 0.18\n",
      "               This parameter has been deprecated in favor of ``labels`` in\n",
      "               version 0.18 and will be removed in 0.20. Use ``labels`` instead.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_similarity_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss correspond to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes the individual\n",
      "        labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss. When\n",
      "        normalized over samples, the Hamming loss is always between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized)\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array, optional, default None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero.\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision  # doctest: +ELLIPSIS\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          1.0...\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_similarity_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the sum of the Jaccard similarity coefficient\n",
      "            over the sample set. Otherwise, return the average of Jaccard\n",
      "            similarity coefficient.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the average Jaccard similarity\n",
      "            coefficient, else it returns the sum of the Jaccard similarity\n",
      "            coefficient over the sample set.\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equivalent\n",
      "        to the ``accuracy_score``. It differs in the multilabel classification\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_similarity_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> jaccard_similarity_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> jaccard_similarity_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),        np.ones((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score)\n",
      "        Compute ranking-based average precision\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, sample_weight=None)\n",
      "        Compute Ranking loss measure\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array or sparse matrix, shape = [n_samples, n_labels]\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array, shape = [n_samples, n_labels]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of the true labels given a probabilistic classifier's\n",
      "        predictions. The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label yt in {0,1} and\n",
      "        estimated probability yp that yt = 1, the log loss is\n",
      "        \n",
      "            -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, optional (default=None)\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "    \n",
      "    make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in GridSearchCV\n",
      "        and cross_val_score. It takes a score function, such as ``accuracy_score``,\n",
      "        ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable,\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : boolean, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : boolean, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "        needs_threshold : boolean, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC)\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary (two-class) classifications. It takes into\n",
      "        account true and false positives and negatives and is generally regarded as\n",
      "        a balanced measure which can be used even if the classes are of very\n",
      "        different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], default None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <http://dx.doi.org/10.1093/bioinformatics/16.5.412>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <http://www.sciencedirect.com/science/article/pii/S1476927104000799>`_\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        -0.33...\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([ 0.5,  1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.849...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']\n",
      "            or array-like of shape (n_outputs)\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([ 0.416...,  1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.824...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average']             or array-like of shape = (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([ 0.004...,  0.083...])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.060...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred)\n",
      "        Median absolute error regression loss\n",
      "        \n",
      "        Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels of\n",
      "        the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^|U| \\sum_{j=1}^|V| \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        contingency : {None, array, sparse matrix},\n",
      "                      shape = [n_classes_true, n_classes_pred]\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_mutual_info_score: Adjusted against chance Mutual Information\n",
      "        normalized_mutual_info_score: Normalized Mutual Information\n",
      "    \n",
      "    ndcg_score(y_true, y_score, k=5)\n",
      "        Normalized discounted cumulative gain (NDCG) at rank K.\n",
      "        \n",
      "        Normalized Discounted Cumulative Gain (NDCG) measures the performance of a\n",
      "        recommendation system based on the graded relevance of the recommended\n",
      "        entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal\n",
      "        ranking of the entities.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (true labels represended as integers).\n",
      "        y_score : array, shape = [n_samples, n_classes]\n",
      "            Predicted probabilities.\n",
      "        k : int\n",
      "            Rank.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> y_true = [1, 0, 2]\n",
      "        >>> y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
      "        >>> ndcg_score(y_true, y_score, k=2)\n",
      "        1.0\n",
      "        >>> y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
      "        >>> ndcg_score(y_true, y_score, k=2)\n",
      "        0.66666666666666663\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Kaggle entry for the Normalized Discounted Cumulative Gain\n",
      "               <https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain>`_\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred)\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is an normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mustual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Index\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n",
      "            against chance)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', n_jobs=1, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix inputs.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features], optional\n",
      "            An optional second feature array. Only allowed if metric != \"precomputed\".\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      "            used at all, which is useful for debugging. For n_jobs below -1,\n",
      "            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      "            are used.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, axis=1, metric='euclidean', batch_size=500, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        Y : array-like\n",
      "            Arrays containing points. Respective shapes (n_samples1, n_features)\n",
      "            and (n_samples2, n_features)\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
      "              'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
      "              'sqeuclidean', 'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            To reduce memory consumption over the naive solution, data are\n",
      "            processed in batches, comprising batch_size rows of X and\n",
      "            batch_size rows of Y. The default value is quite conservative, but\n",
      "            can be changed for fine-tuning. The larger the number, the larger the\n",
      "            memory usage.\n",
      "        \n",
      "        metric_kwargs : dict\n",
      "            keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, axis=1, metric='euclidean', batch_size=500, metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples1, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix}, shape (n_samples2, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, optional, default 1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : string or callable, default 'euclidean'\n",
      "            metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
      "              'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
      "              'sqeuclidean', 'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        batch_size : integer\n",
      "            To reduce memory consumption over the naive solution, data are\n",
      "            processed in batches, comprising batch_size rows of X and\n",
      "            batch_size rows of Y. The default value is quite conservative, but\n",
      "            can be changed for fine-tuning. The larger the number, the larger the\n",
      "            memory usage.\n",
      "        \n",
      "        metric_kwargs : dict, optional\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : numpy.ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', filter_params=False, n_jobs=1, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are::\n",
      "            ['rbf', 'sigmoid', 'polynomial', 'poly', 'linear', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "        \n",
      "        Y : array [n_samples_b, n_features]\n",
      "            A second feature array only if X has shape [n_samples_a, n_features].\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        filter_params : boolean\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
      "            used at all, which is useful for debugging. For n_jobs below -1,\n",
      "            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
      "            are used.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold.  This ensures that the graph starts on the\n",
      "        x axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True targets of binary classification in range {-1, 1} or {0, 1}.\n",
      "        \n",
      "        probas_pred : array, shape = [n_samples]\n",
      "            Estimated probabilities or decision function.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : array, shape = [n_thresholds + 1]\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : array, shape = [n_thresholds + 1]\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))]\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision  # doctest: +ELLIPSIS\n",
      "        array([ 0.66...,  0.5       ,  1.        ,  1.        ])\n",
      "        >>> recall\n",
      "        array([ 1. ,  0.5,  0.5,  0. ])\n",
      "        >>> thresholds\n",
      "        array([ 0.35,  0.4 ,  0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None)\n",
      "        Compute precision, recall, F-measure and support for each class\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, 1.0 by default\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None (default), 'binary', 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : int (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE\n",
      "        (array([ 0. ,  0. ,  0.66...]),\n",
      "         array([ 0.,  0.,  1.]),\n",
      "         array([ 0. ,  0. ,  0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the precision\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS\n",
      "        array([ 0.66...,  0.        ,  0.        ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        R^2 (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a R^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples), optional\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : string in ['raw_values', 'uniform_average', 'variance_weighted'] or None or array-like of shape (n_outputs)\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The R^2 score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, R^2 score may be negative (it need not actually\n",
      "        be the square of a quantity R).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred, multioutput='variance_weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.938...\n",
      "        >>> y_true = [1,2,3]\n",
      "        >>> y_pred = [1,2,3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1,2,3]\n",
      "        >>> y_pred = [2,2,2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1,2,3]\n",
      "        >>> y_pred = [3,2,1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the recall\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([ 1.,  0.,  0.])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, average='macro', sample_weight=None)\n",
      "        Compute Area Under the Curve (AUC) from prediction scores\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task in label indicator format.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            True binary labels in binary label indicators.\n",
      "        \n",
      "        y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve\n",
      "        \n",
      "        roc_curve : Compute Receiver operating characteristic (ROC)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> roc_auc_score(y_true, y_scores)\n",
      "        0.75\n",
      "    \n",
      "    roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC)\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        y_true : array, shape = [n_samples]\n",
      "            True binary labels in range {0, 1} or {-1, 1}.  If labels are not\n",
      "            binary, pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array, shape = [n_samples]\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label considered as positive and others are considered negative.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : boolean, optional (default=True)\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : array, shape = [>2]\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        tpr : array, shape = [>2]\n",
      "            Increasing true positive rates such that element i is the true\n",
      "            positive rate of predictions with score >= thresholds[i].\n",
      "        \n",
      "        thresholds : array, shape = [n_thresholds]\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        roc_auc_score : Compute Area Under the Curve (AUC) from prediction scores\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([ 0. ,  0.5,  0.5,  1. ])\n",
      "        >>> tpr\n",
      "        array([ 0.5,  0.5,  1. ,  1. ])\n",
      "        >>> thresholds\n",
      "        array([ 0.8 ,  0.4 ,  0.35,  0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "                 label values for each sample\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`. If X is\n",
      "            the distance array itself, use \"precomputed\" as the metric.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array, shape = [n_samples]\n",
      "            Silhouette Coefficient for each samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <http://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 <= n_labels <= n_samples - 1.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,              [n_samples_a, n_features] otherwise\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array, shape = [n_samples]\n",
      "             Predicted labels for each sample.\n",
      "        \n",
      "        metric : string, or callable\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If X is the distance\n",
      "            array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int or None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            The generator used to randomly select a subset of samples.  If int,\n",
      "            random_state is the seed used by the random number generator; If\n",
      "            RandomState instance, random_state is the random number generator; If\n",
      "            None, the random number generator is the RandomState instance used by\n",
      "            `np.random`. Used when ``sample_size is not None``.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <http://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score`.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array, shape = [n_samples]\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          ...                                                  # doctest: +ELLIPSIS\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_similarity_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    c:\\python36\\lib\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzCMSExZGIZhGJGYsjAMwzAi+f8BRjHtBjq/F4EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcd35438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax=plt.subplots(2,sharex=\"col\")\n",
    "x=range(len(x_test))\n",
    "ax[0].scatter(x,y_test)\n",
    "ax[0].scatter(x,y_pred)\n",
    "\n",
    "ax[1].bar(x,y_pred-y_test)\n",
    "ax[1].set_ylim([-0.5,0.5])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_err=y_test-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "green\n",
      "red\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "red\n",
      "green\n",
      "green\n"
     ]
    }
   ],
   "source": [
    "for y in y_err:\n",
    "    if y>0:\n",
    "        print(\"red\")\n",
    "    else:\n",
    "        print(\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
